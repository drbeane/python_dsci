{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search \n",
    "\n",
    "Grid search is a method for performing hyperparameter tuning for a model. This technique involves identifying one or more hyperparameters that you would like to tune, and then selecting some number of values to consider for each hyperparameter. We then evaluate each possible set of hyperparameters by performing some type of validation. Typically, this will involve performing cross-validation to generate an out-of-sample performance estimate for each set of hyperparameters. We then typically select the model that has the highest cross-validation score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "\n",
    "We will illustrate how to perform grid search in Scikit-Learn in this lesson. We begin by importing a few packages and tools that are not directly related to grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data\n",
    "\n",
    "In this lesson, we will work with a synthetic dataset created for a classification problem. The dataset contains 400 observations, each of which will have 6 features, and will be assigned one of 10 possible classes. The features are stored in an array named `X`, while the labels are stored in an array named `y`. We start by viewing the contents of `X` in a DataFrame format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X, y = make_classification(n_samples=400, n_features=6, n_informative=6, \n",
    "                           n_redundant=0, n_classes=10, class_sep=2)\n",
    "\n",
    "pd.DataFrame(X)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now view the first few elements of `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search in Scikit-Learn\n",
    "\n",
    "GridSearch is performed in Scikit-Learn using the `GridSearchCV` class. We will import this class in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search with Logistic Regression\n",
    "\n",
    "We will illustrate the usage of `GridSearchCV` by first performing hyperparameter tuning to select the optimal value of the regularization parameter `C` in a logistic regression model. \n",
    "\n",
    "We start by defining a parameter grid. This is a dictionary containing keys for any hyperparameters we wish to tune over. The values associated with each key should be a list or array of values to consider for that hyperparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'C': 10**np.linspace(-3,3,20)}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create an instance of the estimate that we wish to tune over. In this case, that is the `LogisticRegression` class. Note that we do not fit the model to the training data yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create an instance of the `GridSearchCv` class. When creating this instance, we must provide an estimator, a parameter grid, a number of folds to use in cross-validation, an evaluation metric to use in cross-validation. If we specify that `refit=True` (which is the default value), then `GridSearchCV` will automatically fit the best model found to the entire data set. We will discuss this more later. \n",
    "\n",
    "After creating an instance of `GridSearchCV`, we train it using the `fit` method.\n",
    "\n",
    "A trained `GridSearchCV` obtain has many attributes and methods that we might be interested in. We will explore this in more detail later, but for now, the most important attributes are `best_score_` and `best_params_`. The `best_score_` attribute will contain the cross-validation score for the best model found, while `best_params_` will be a dictionary of the hyperparameter values that generated the optimal cross-validation score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gridsearch = GridSearchCV(lin_reg, param_grid, cv=10, scoring='accuracy', \n",
    "                             refit=True, iid=False)\n",
    "lr_gridsearch.fit(X, y)\n",
    "\n",
    "print(lr_gridsearch.best_score_)\n",
    "print(lr_gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the highest cross-validation score obtain for any of the values of `C` considered was 62.7%. This was obtained by using `C = 0.16237767`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the Best Model\n",
    "\n",
    "When trained, `GridSearchCV` class will automatically refit a final model to the full training set using the optimal hyperparameter values found. This model is stored in the attribute `best_estimator_`. \n",
    "\n",
    "In the cell below, we extract the best model from our `GridSearchCV` object and use it to calculate the training accuracy for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr_gridsearch.best_estimator_\n",
    "print('Training Score:', lr_model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search with Decision Trees\n",
    "\n",
    "We will now illustrate how to use `GridSearchCV` to perform hyperparameter tuning for a decision tree. We will tune over two hyperparameters: `max_depth` and `min_samples_leaf`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "    'max_depth': [2, 4, 8, 16, 32, 64], \n",
    "    'min_samples_leaf': [2, 4, 8, 16]\n",
    "}]\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "np.random.seed(1)\n",
    "dt_gridsearch = GridSearchCV(tree, param_grid, cv=10, scoring='accuracy', \n",
    "                             refit=True, iid=False)\n",
    "dt_gridsearch.fit(X, y)\n",
    "\n",
    "print(dt_gridsearch.best_score_)\n",
    "print(dt_gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree with the highest cross-validation score had a `max_depth` of 32 and a `min_samples_leaf` of 8. Notice that this model outperforms the best logistic regression model that we found above. In the cell below, we extract the best model from the `GridSearchCV` object, and calculate its score on the training set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = dt_gridsearch.best_estimator_\n",
    "print('Training Score:', dt_model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search with Random Forests\n",
    "\n",
    "We will now illustrate how to use `GridSearchCV` to perform hyperparameter tuning for a random forest. We will tune over two hyperparameters: `max_depth` and `min_samples_leaf`. We will set the `n_estimators` hyperparameter to 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "    'max_depth':[2, 4, 8, 16, 32, 64], \n",
    "    'min_samples_leaf':[2, 4, 8, 16]\n",
    "}]\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "np.random.seed(1)\n",
    "rf_gridsearch = GridSearchCV(forest, param_grid, cv=10, scoring='accuracy',\n",
    "                             refit=True, iid=False)\n",
    "rf_gridsearch.fit(X, y)\n",
    "\n",
    "print(rf_gridsearch.best_score_)\n",
    "print(rf_gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest with the highest cross-validation score had a `max_depth` of 8 and a `min_samples_leaf` of 4. This model outperforms either of our previous two models. In the cell below, we extract the best model from the `GridSearchCV` object, and calculate its score on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf_gridsearch.best_estimator_\n",
    "print('Training Score:', rf_model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Grid Search Results\n",
    "\n",
    "If we would like to see more detailed results pertaining to the results of the grid seach process, more information can be found in the `cv_results` attribute of a trained instance of the `GridSearchCV` class. This attribute contains a dictionary with several pieces of information pertaining to the results of the cross-validation steps. We will start by looking at the keys of the items stored in this dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = rf_gridsearch.cv_results_\n",
    "print(cv_res.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The items `split0_test_score` through `split9_test_score` each contain the validation score for each of the models considered on one particular fold. The average validation scores for each individual model can be found in the `mean_test_score` item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_res['mean_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we print the average test scores along with the hyperparameter values for the models that generated them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score, params in zip(cv_res['mean_test_score'], cv_res['params']):\n",
    "    print(score, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that although the `max_depth=8`, `min_samples_leaf=4` model performed the best, there were a few other models that had very similar results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
