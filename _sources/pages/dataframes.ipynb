{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "The pandas package provides us with the DataFrame data type for working with structured data, or in other words, data that is organized in a tabular format with defined rows and columns. \n",
    "\n",
    "We have previously seen two tools that can be used for working with structured data: 2D Numpy arrays and dictionaries. DataFrames provide the following advantages over these other data structures:\n",
    "\n",
    "1. The elements of a Numpy array must all be of the same data type. While individual columns of a pandas DataFrame must have a consistent data type, different columns can contain different types. \n",
    "\n",
    "2. Unlike a Numpy array, we can assign names to the columns and (less importantly) to the rows of a DataFrame. \n",
    "\n",
    "3. We have seen that we can use dictionaries to represent tabular data. For example, we can set the values in a dictionary to be lists representing columns, in which case the keys will represent column names. However, there would be no explicit concept of a row in such a setup. DataFrames explicitly define both rows and columns. \n",
    "\n",
    "It is a common convention to import pandas under the alias `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame from a Dictionary\n",
    "\n",
    "We can create a pandas DataFrame from a dictionary in which the key-value pairs represent columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_dict = {\n",
    "    'eid':[214, 174, 126, 227, 151, 175, 193, 146],\n",
    "    'name':['Drew', 'Faye', 'Beth', 'Chad', 'Hana', 'Gary', 'Alex', 'Emma'],\n",
    "    'age':[25, 19, 42, 31, 25, 28, 31, 25],\n",
    "    'rate':[11.50, 12.30, 14.60, 12.50, None, 15.60, 13.50, 14.30],\n",
    "    'hours':[38, 32, 40, 29, 40, 36, 24, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = pd.DataFrame(employee_dict)\n",
    "employee_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every DataFrame comes with a `head()` method that can be used to view the first few rows of the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Elements\n",
    "\n",
    "Every row and column in a pandas DataFrames has both a name, and a numerical index. We can use the `iloc[]` attribute to access DataFrame elements using column and row indices, and we can use `loc[]` to access elements using column and row names. \n",
    "\n",
    "Note that for the DataFrame we have created above, the numerical indices for the rows are the same as their names. This is common, but not required. We will see an example later where the row names are different from the numerical indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.iloc[2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.iloc[4, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.loc[4, 'name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Rows\n",
    "\n",
    "We can extract entire rows from a DataFrame using slicing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.loc[4,:]\n",
    "#employee_df.loc[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Columns\n",
    "\n",
    "We can also use slicing to extract entire columns from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.loc[:,'name']\n",
    "#employee_df['name']\n",
    "#employee_df.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `loc` and `iloc` indexing attributes support fancy indexing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.loc[:,['name','rate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since columns and rows have a well-defined order in a DataFrame, we can also use slicing with the `loc` indexer to slice a range of columns using their column names. \n",
    "\n",
    "Notice that when performing this type of slicing, the column appearing after the colon IS included in the slice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.loc[:,'eid':'age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding New Columns\n",
    "\n",
    "We can use `loc` to add new columns to a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.loc[:,'pay'] = employee_df.loc[:,'rate'] * employee_df.loc[:,'hours']\n",
    "employee_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Operations\n",
    "\n",
    "DataFrames come equipped with several methods such as `sum()` and `mean()` for performing operations on columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.loc[:,['hours', 'pay']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.iloc[:,2:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "A common headache for anyone who works with data is encountering data sets with missing values. Pandas provides us with several tools for identifying and working with missing values. We will discuss some of those here. \n",
    "\n",
    "The `isnull()` method returns a DataFrame consisting of Boolean values that indicate the location of missing values within the original DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the `isnull()` and `sum()` methods to count the number of missing values in each row of a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dropna()` method removes from a DataFrame any rows that contain missing values. By default, this method returns a new DataFrame, leaving the original object untouched. However, if we set the parameter `inplace=True`, then the operation is performed on the original DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.dropna(inplace=True)\n",
    "employee_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "We can use Boolean masking to filter DataFrames, just as with Numpy arrays.\n",
    "\n",
    "The code in the cell below filters `employee_df`, keeping only records for employees who worked more than 30 hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = employee_df.loc[:,'hours'] > 30\n",
    "employee_df.loc[sel, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call above that the average age for employees in the data set was 28.25. In the cell below, we determine the average age of employees older than 25. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = employee_df.loc[:,'age'] > 25\n",
    "employee_df.loc[sel, 'age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with Numpy arrays, we can use & and | to filter on multiple criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = (employee_df.loc[:,'rate'] < 13) & (employee_df.loc[:,'hours'] > 30)\n",
    "employee_df.loc[sel, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting a New Index\n",
    "\n",
    "DataFrames refer to the collection of row names (somewhat confusingly) as the **index** of the DataFrame. In most cases, these row names will be set to be equal to the numerical indices of the rows. However, it is possible to set the \"index\" of a DataFrame to a column within a DataFrame \n",
    "\n",
    "In the cell below, we set `eid` to be the index of `employee_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.set_index('eid', inplace=True)\n",
    "employee_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still use `iloc` to index rows and columns according to their numerical indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.iloc[:4, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we must use the correct row names when using `loc` to subset the DataFrame. Notice that the rows are kept in their original order, and are not reordered according to the new row names that we have set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.loc[:227, :'rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Records to a Data Frame\n",
    "\n",
    "We will now see how to add new records to a DataFrame. First, lets recall what `employee_df` DataFrame currently contains. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `loc` to add a new records with a specific row name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.loc[232] = ['Iris', 34, 11.2, 30, 11.2 * 30]\n",
    "employee_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a DataFrame of new records with the same structure as our original DataFrame, and then combine the two using the `append()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_records = pd.DataFrame({\n",
    "    'name':['Jake', 'Kate'],\n",
    "    'age':[36, 29],\n",
    "    'rate':[11.7, 12.4],\n",
    "    'hours':[34, 32],\n",
    "}, index=[251, 368])\n",
    "\n",
    "new_records.loc[:,'pay'] = new_records.loc[:,'rate'] * new_records.loc[:,'hours']\n",
    "\n",
    "new_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.append(new_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that `append()` returns a new DataFrame. It does not change either of the DataFrames being combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrame from List of Lists\n",
    "\n",
    "There are many ways to create DataFrames. At the beginning of this lesson, we saw how to create DataFrames from dictionaries. DataFrames can also be created from lists of lists. In this case, row and column names must be provided, or they will be set to be the same as the numerical indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoL = [[30, 3.7, 'Ant', True], \n",
    "       [24, 1.3, 'Bird', True], \n",
    "       [45, 2.6, 'Cat', False], \n",
    "       [18, 4.2, 'Dog', True]]\n",
    "\n",
    "unnamed_df = pd.DataFrame(LoL)\n",
    "print(unnamed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_df = pd.DataFrame(data = LoL,\n",
    "                        index = ['Row1', 'Row2', 'Row3', 'Row4'],\n",
    "                        columns = ['Col1', 'Col2', 'Col3', 'Col4'])\n",
    "print(named_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data From a Text File\n",
    "\n",
    "Tabular data is often stored in the form of text files. When a text file is used to store tabular data, each record (row) in the data set is recorded on a separate line of the text file. Elements belonging to different columns are separated by a specific sequence of one or more characters. This sequence is referred to as a **separator** or **delimiter**. Common choices for a **delimiter** are tabs, commas, and spaces. A data file using tabs as delimiters is often referred to as being **tab-separated** or **tab-delimited**. Similarly, a data file using commas for delimiters is often referred to as being **comma-separated** or **comma-delimited**. The extension used for a text file storing data can be essentially anything, but it is common to see `.txt` files used. When the file is comma-delimited, the file will often have the extension `.csv`, which stands for \"comma-separated values\". \n",
    "\n",
    "Pandas provides several tools for reading data from text files into DataFrames. Two common functions used for this task are `pd.read_table()` and `pd.read_csv()`. These functions have several parameters that control how the data is imported, but for many use-cases, it sufficient to use just two parameters: `filepath_or_buffer` and `sep`. \n",
    "\n",
    "* The `filepath_or_buffer` parameter expects a string that states the path to the data file. The path can be an absolute path that states exactly where the file lives on your system, or a relative path that explains where the file is stored in relation to the directory containing the notebook or script you are using. Relative paths tend to be more flexible, as they allow the directory containing the notebook and data file to be moved without having to update the path in the code. \n",
    "\n",
    "* The `sep` parameter specify the separator or delimiter used in the data file. The default value of `sep` is `'\\t'` for `pd.read_table()`, and is `','` for `pd.read_csv()`.\n",
    "\n",
    "These two functions can be used interchangeably, as long as you are careful to specify the correct separator for your data file. \n",
    "\n",
    "We will now provide two examples of loading data into a Pandas DataFrame. \n",
    "\n",
    "### Pima Diabetes Dataset\n",
    "\n",
    "We will start by loading the Pima Indian Diabetes dataset. This data set contains information collected in a study on the prevalence of type 2 diabetes in a Native American community in Arizona. You can find more information about this dataset here: [Pima Indian Diabetes](https://www.andreagrandi.it/2018/04/14/machine-learning-pima-indians-diabetes/). \n",
    "\n",
    "The data is stored in a comma-separated file named `diabetes.csv` stored within the `data/` directory. In the cell below, we load this dataset into a DataFrame and then view its first few rows. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_table('data/diabetes.csv', sep=',')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic Dataset\n",
    "\n",
    "In the next example, we will load the Titanic dataset. This dataset contains information about the 887 passengers on the first and only voyage of the HMS Titanic. You can read more about this dataset here: [Titanic Dataset](https://www.kaggle.com/c/titanic).\n",
    "\n",
    "The data is store in the tab-delimited file `titanic.txt` stored within the `data/` directory. In the cell below, we load this dataset into a DataFrame and then view its first few rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_table('data/titanic.txt', sep='\\t')\n",
    "titanic.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
